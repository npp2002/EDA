---
title: "Model for Q2"
output: html_document
date: "2023-04-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=F)
options(scipen=999)
library(tidyverse)    #Essential Functions
library(caret)
```

## PREPPING/VISUALIZING DATASET

Is the relationship between release_year and genre a good predictor of revenue?

```{r}
#find frequency of each genre type 
popular <- steam_games_final %>% 
   separate_rows(genres,sep=";") %>% 
   count(genres) %>%
   arrange(desc(n))


steam_games_genre_grep = filter(steam_games_final,revenue!=0) %>%
  mutate(is.action=grepl("Action",genres)) %>%
  mutate(is.indie=grepl("Indie",genres)) %>%
  mutate(is.casual=grepl("Casual",genres)) %>%
  mutate(is.strategy=grepl("Strategy",genres)) %>%
  mutate(is.adventure=grepl("Adventure",genres)) %>%
  mutate(is.simulation=grepl("Simulation",genres)) %>%
  mutate(is.rpg=grepl("RPG",genres)) %>%
  mutate(is.earlyaccess=grepl("Early Access",genres)) %>%
  mutate(is.freetoplay=grepl("Free to Play",genres)) %>%
  mutate(is.sports=grepl("Sports",genres)) %>%
  mutate(log_revenue=log(revenue))

steam_genres_top = steam_games_genre_grep %>%
  gather(is.action:is.sports, key="Top_Genre", value="Present")

sgt = steam_genres_top %>%
  filter(Present==TRUE)

#descriptive graph of question
sgt_revenue <- sgt %>% 
  group_by(release_year, Top_Genre) %>% 
  summarize(total_revenue = sum(revenue))

ggplot(aes(x = release_year, y = total_revenue), data = sgt_revenue) + 
  geom_bar(aes(fill = Top_Genre), stat = "identity") +
  scale_x_continuous(breaks = seq(min(sgt$release_year), max(sgt$release_year), 1)) +
  xlab("Release Year") +
  ylab("Total Revenue") +
  ggtitle("Top 10 Genre Change over Time vs. Revenue") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


##TRAIN AND TEST
```{r}
steam_games_test_train = steam_games_genre_grep %>%
  mutate(Set = sample(c("Train","Test"),size=nrow(steam_games_genre_grep),replace=TRUE,prob=c(0.8,0.2)))

train.steam = filter(steam_games_test_train,Set=="Train")
test.steam = filter(steam_games_test_train,Set=="Test")

```


## LINEAR MODELING
```{r}
lm1 = lm(revenue~is.action+is.indie+is.casual+is.strategy+is.simulation,data=train.steam)
lm2 = lm(revenue~is.action+is.indie+is.casual+is.strategy+is.simulation+is.rpg+is.earlyaccess+is.freetoplay+is.sports,data=train.steam)
lm3 = lm(revenue~is.action+is.indie+is.casual+is.strategy+is.simulation+release_year,data=train.steam)
lm4 = lm(revenue~is.action+is.indie+is.casual+release_year,data=train.steam)
lm5 = lm(revenue~is.action+is.indie+release_year,data=train.steam)
lm6 = lm(revenue~is.action+is.indie+release_year+achievements,data=train.steam)
lm7 = lm(log_revenue~is.action+is.indie+is.casual+release_year,data=train.steam)
lm8 = lm(log_revenue~is.action+is.indie+is.casual+is.strategy+release_year,data=train.steam)
lm9 = lm(log_revenue~is.action+is.indie+is.casual+is.strategy+is.simulation+release_year,data=train.steam)

test.steam2 = test.steam %>% 
  add_predictions(lm1,var="lm1") %>%
  add_predictions(lm2,var="lm2") %>%
  add_predictions(lm3,var="lm3") %>%
  add_predictions(lm4,var="lm4") %>%
  add_predictions(lm5,var="lm5") %>%
  add_predictions(lm6,var="lm6") %>%
  add_predictions(lm7,var="lm7") %>%
  add_predictions(lm8,var="lm8") %>%
  add_predictions(lm9,var="lm9") %>%
  add_residuals(lm1,var="lm1.res") %>%
  add_residuals(lm2,var="lm2.res") %>%
  add_residuals(lm3,var="lm3.res") %>%
  add_residuals(lm4,var="lm4.res") %>%
  add_residuals(lm5,var="lm5.res") %>%
  add_residuals(lm6,var="lm6.res") %>%
  add_residuals(lm7,var="lm7.res") %>%
  add_residuals(lm8,var="lm8.res") %>%
  add_residuals(lm9,var="lm9.res")
  

MAE.func <- function(residual){
  LOSS = mean(abs(residual))
  return(LOSS)
}

RMSE.func <- function(residual){
  LOSS <- sqrt(mean(residual^2))
}

#make html table for this

MODELS=c("MODEL 1","MODEL 2","MODEL 3","MODEL 4","MODEL 5","MODEL 6","MODEL 7 (log)","MODEL 8 (log)","MODEL 9 (log)")
MAE=c(MAE.func(test.steam2$lm1.res),
      MAE.func(test.steam2$lm2.res),
      MAE.func(test.steam2$lm3.res),
      MAE.func(test.steam2$lm4.res),
      MAE.func(test.steam2$lm5.res),
      MAE.func(test.steam2$lm6.res),
      MAE.func(test.steam2$lm7.res),
      MAE.func(test.steam2$lm8.res),
      MAE.func(test.steam2$lm9.res))
RMSE=c(RMSE.func(test.steam2$lm1.res),
       RMSE.func(test.steam2$lm2.res),
       RMSE.func(test.steam2$lm3.res),
       RMSE.func(test.steam2$lm4.res),
       RMSE.func(test.steam2$lm5.res),
       RMSE.func(test.steam2$lm6.res),
       RMSE.func(test.steam2$lm7.res),
       RMSE.func(test.steam2$lm8.res),
       RMSE.func(test.steam2$lm9.res))
COMPARE=tibble(MODELS=MODELS,RMSE=RMSE,MAE=MAE)
print(COMPARE)

#what I am noticing - it doesn't seem like genre is a good predictor variable. This could also be attributed to the test and train set, which is skewing data. Because a majority of games are released after 2000, the train set could potentially have more recent releases compared to the test. Let's try cross validation on a polynomial model?
```
Interpreting the results for Model 7. Since we took the log of our target variable, revenue, the RMSE and MAE don't represent the actual error in predicting the revenue. In order to find the actual RMSE and MAE from the log of each, you must take the exponent of each value and subtract 1 (log of 0) from each. 

##GRAPHING AND EVALUATING MODEL 7
```{r}

#evaluating model 5
predicted1 <- predict(lm5, newdata = test.steam)
actual_and_predict <- data.frame(actual = test.steam$revenue, predicted = predicted1)

ggplot(actual_and_predict, aes(x = actual, y = predicted)) +
  geom_point(alpha=0.5) +
  scale_x_log10(labels = scales::dollar_format(prefix = "$")) +
  scale_y_log10(labels = scales::dollar_format(prefix = "$")) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(x = "Actual Revenue", y = "Predicted Revenue") +
  ggtitle("Actual vs. Predicted Revenue for MODEL 5")


#evaluating model 7
predicted2 <- predict(lm9, newdata = test.steam)
actual_and_predict <- data.frame(actual = exp(test.steam$log_revenue), predicted = exp(predicted2))

ggplot(actual_and_predict, aes(x = actual, y = predicted)) +
  geom_point(alpha=0.5) +
  scale_x_log10(labels = scales::dollar_format(prefix = "$")) +
  scale_y_log10(labels = scales::dollar_format(prefix = "$")) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(x = "Actual Revenue", y = "Predicted Revenue") +
  ggtitle("Actual vs. Predicted Revenue for MODEL 9")


```

**Interpretation:** As seen by the graphs above, it is evident that MODEL 7 fits the revenue better than MODEL 5. This can be attributed to taking the logarithm of the target variable, revenue. By taking the log of the target variable, a non-linear relationship can be converted into a linear relationship. There are also a lot of outliers evident in revenue because the price of a select few games are much higher than others, affecting the revenue. Taking the log of revenue downweights the effect of such outliers, reducing extreme value impact and making the model more attuned. 






## INTERACTION EFFECTS 

```{r}
GENRE_TOP = steam_genres_top %>% 
  mutate(revenue=revenue/1000000) %>%
  filter(Present == TRUE, Top_Genre %in% c("is.action", "is.indie", "is.casual")) %>%
  dplyr::select(Top_Genre, revenue, release_year) %>%
  mutate(Top_Genre = factor(Top_Genre))

ctrl <- trainControl(method = "cv", number = 5)
cmodel1=lm(revenue~release_year,GENRE_TOP)
tidy(model1)
model2=lm(revenue~release_year+Top_Genre,GENRE_TOP)
tidy(model2)
model3=lm(revenue~release_year+Top_Genre+release_year*Top_Genre,GENRE_TOP)
tidy(model3)
```
interaction: when you don't include interaction in model, genre+rleease year 

```{r}
revenue_pred = GENRE_TOP %>%
  gather_predictions(model1,model2,model3) %>%
  glimpse()
```



```{r}
GENRE_TOP %>%
  gather_predictions(model1,model2,model3) %>%
  ggplot() +
    geom_point(aes(x=release_year,y=revenue,color=Top_Genre)) +
    geom_line(aes(x=release_year,y=pred,color=Top_Genre),size=1)+
    theme_minimal()+
    facet_grid(model~.) +
    ylim(0,100) +
    xlab("Release Year") +
    ylab("Revenue (In Millions)")
```
```{r}

RMSE_int = function(residual){
  return (sqrt(mean(residual^2)))
}

MAE_int = function(residual){
  return (abs(mean(residual^2)))
}
# Predicted values for each model
pred1 <- predict(model1)
pred2 <- predict(model2)
pred3 <- predict(model3)
pred4 <- predict(model4)

# Residuals for each model
resid1 <- GENRE_TOP$revenue - pred1
resid2 <- GENRE_TOP$revenue - pred2
resid3 <- GENRE_TOP$revenue - pred3
resid4 <- GENRE_TOP$revenue - pred4



MODELS=c("MODEL 1","MODEL 2","MODEL 3")
RMSE=c(RMSE_int(resid1),
      RMSE_int(resid2),
      RMSE_int(resid3))
MAE=c(MAE_int(resid1),
      MAE_int(resid2),
      MAE_int(resid3))
COMPARE=tibble(MODELS=MODELS,RMSE=RMSE,MAE=MAE)

# Convert tibble to HTML table
COMPARE_table <- COMPARE %>%
  kable(format = "html") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE) 

COMPARE_table
```
**INTERPRETATION**: Based on the RMSE and MAE, the loss functions are not minimized as optimally as we'd like. Genre is a very complicated predictor variable that can't be restricted to a black or white approach. Genre is an ever-evolving variable which is directly impacted by market trends over time. However, just because the model has high loss functions, this does not take away from the fact that there is an interesting relationship developing within the interaction model.

```{r}
GENRE_TOP$is.action <- ifelse(GENRE_TOP$Top_Genre == 'is.action', 1, 0)
GENRE_TOP$is.casual <- ifelse(GENRE_TOP$Top_Genre == 'is.casual', 1, 0)
GENRE_TOP$is.indie <- ifelse(GENRE_TOP$Top_Genre == 'is.indie', 1, 0)

modelInt <- lm(revenue ~ release_year*is.action + release_year*is.indie + is.casual + release_year*is.casual, data = GENRE_TOP)

anova(modelInt)
```
**INTERPRETATION**: In order to understand the statistical significance of each genre on revenue trends over time, I created a model that fits separate interactions between release_year and each top three genre (indie, casual, action). The anova summary shows that release_year and is.action have a significant impact on revenue. The interaction between is.action and release_year also have a low P-value. The interaction between release_year and indie games is also very low, but not as low as action games. Casual games and their relationship with release_year we're not available in the anova summary, which probably means the relationship between the two are insignificant. 

The anova test also proves that there IS a statistical significance between action games and their decrease in revenue over time

**REAL WORLD INTERPRETATION: WHY?** As stated previously, the relationship between video game genres and revenue over time is complicated. If we ignore the impact of inflation and hyperproduction of games in the past decade or so, we noticed a few things:
1. indie games tend to stay consist, revenue wise, over time. This is very interesting; indie games are characterized by an analog, or vintage, aesthetic and can sometimes emulate the old days. For example, indie games have lower fps and high pixelation. examples include (INSERT EXAMPLES FROM A FEW DECADES AGO AND FROM A FEW YEARS AGO TOO)

```{r}
set.seed(216)


```



##RIDGE REGRESSION USING RELEASE_YEAR GENRES AS PREDICTORS 

```{r}
library(glmnet)
response_var <- steam_games_genre_grep$revenue
predictor_vars <- steam_games_genre_grep[,c("is.action","is.indie","is.casual","is.strategy","is.simulation","is.rpg","is.earlyaccess","is.freetoplay","is.sports","release_year")]
predictor_vars.MATRIX <- as.matrix(predictor_vars)
ridge_mod <- glmnet(x=predictor_vars.MATRIX,y=response_var,alpha=0)
plot(ridge_mod,xvar="lambda")

legend("topright", legend=colnames(predictor_vars.MATRIX), col=1:ncol(predictor_vars.MATRIX), lwd=2)
```

##LASSO REGRESSION

```{r}
response_var3 <- steam_games_genre_grep$revenue
predictor_vars3 <- steam_games_genre_grep[,c("is.action","is.indie","is.casual","is.strategy","is.simulation","is.rpg","is.earlyaccess","is.freetoplay","is.sports","release_year")]
predictor_vars3.MATRIX <- as.matrix(predictor_vars3)
lasso.mod <- glmnet(x=predictor_vars3.MATRIX,y=response_var3,alpha=1)
plot(lasso.mod,xvar="lambda",col=1:ncol(predictor_vars3.MATRIX))

legend("topright", legend=colnames(predictor_vars3.MATRIX), col=1:ncol(predictor_vars3.MATRIX), lwd=2)

#WHAT MAKES A LASSO REGRESSION COOL? it can be used to identify the most important variables that are predictive of your outcome variable, and can also help to prevent overfitting by reducing the impact of variables that may be less important.

#you can identify which variables have the highest coefficients (i.e., the most important predictors), and also see how the coefficients change as you increase the strength of the regularization penalty.

#as seen by the graph below, the highest predictors are is.action and is.indie. This makes sense because they are the most popular genres and should indicate 

#cross validation
```







