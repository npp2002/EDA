---
title: "Model0-2"
output: html_document
date: "2023-04-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=F)
options(scipen=999)
library(tidyverse)    #Essential Functions
```

## R Markdown

```{r}
sgf = steam_games_final %>%
  mutate(SPLIT=sample(x=c("TRAIN","TEST"),size=nrow(steam_games_final),
                  replace=T,prob=c(0.85,0.15)))
TRAIN=sgf %>%  filter(SPLIT=="TRAIN")
TEST=sgf %>% filter(SPLIT=="TEST")

TRAIN %>% 
  summarize(mean=mean(price),sd=sd(price),min=min(price),max=max(price))



# define the function to get fitted values
MODEL0 <- function(DATA, COEF) {
  FIT <- COEF[1]
}

# define the MSE and MAE functions
MSE0 <- function(DATA, COEF) {
  ERROR <- DATA$price - MODEL0(DATA, COEF)
  LOSS <- mean(ERROR^2)
  return(LOSS)
}

MAE0 <- function(DATA, COEF) {
  ERROR <- DATA$price - MODEL0(DATA, COEF)
  LOSS <- mean(abs(ERROR))
  return(LOSS)
}

# define the beta0 values
COEF0 <- tibble(
  beta0 = seq(0, 422, length = 100)
)

COEF0 %>% 
  mutate(MSE=purrr::map_dbl(beta0,MSE0,DATA=TRAIN),
         MAE=purrr::map_dbl(beta0,MAE0,DATA=TRAIN),
         rankMSE=rank(MSE),rankMAE=rank(MAE)) %>%
         filter(rankMSE<5,rankMAE<5)

# fit the model using MSE and MAE
BESTMSE0 <- optim(par = 0, fn = MSE0, DATA = TRAIN)
BESTMSE0$par
BESTMAE0 <- optim(par = 0, fn = MAE0, DATA = TRAIN)
BESTMAE0$par

#BESTMSE0$par is the vector of optimal parameters that minimize the mean squared error (MSE) when using the optim() function.

#In this case, BESTMSE0 is the output of the optim() function applied to the MSE0 function, using the TRAIN dataset as input and starting the optimization algorithm with par=0. The optim() function searches for the value of beta0 that minimizes the MSE by iteratively updating beta0 until it converges to the optimal value. The optimal value of beta0 is stored in the par component of the BESTMSE0 object.

LM0=lm(price~1,data=TRAIN)
summary(LM0)
coef(LM0)

```

```{r}
#Model1a 
#Y = Bo + B1X1 + e
#E(Y) = Bo + B1X1
#contains two variables, visualizing price vs. negative proportions

#Model for fitted values 
MODEL1A = function(DATA,COEF){
  FIT = COEF[1]+COEF[2]*DATA$neg_rate_prop
}

MSE1A = function(DATA,COEF){
  ERROR = DATA$price-MODEL1A(DATA,COEF)
  LOSS = mean(ERROR^2)
  return(LOSS)
}

MAE1A = function(DATA,COEF){
  ERROR = DATA$price-MODEL1A(DATA,COEF)
  LOSS = mean(abs(ERROR))
  return(LOSS)
}

COEF1A=tibble(
  beta0=runif(10000,0,500),
  beta1=runif(10000,0,500)
)

COEF1A %>% 
  mutate(MSE=apply(COEF1A,1,MSE1A,DATA=TRAIN),
         MAE=apply(COEF1A,1,MAE1A,DATA=TRAIN),
         rankMSE=rank(MSE),rankMAE=rank(MAE)) %>%
         filter(rankMSE<5,rankMAE<5)


ggplot(data=TRAIN) +
  geom_point(aes(x=neg_rate_prop,y=price),color="lightskyblue2") + theme_dark() +
  geom_abline(aes(intercept=2.22,slope=7.56),color="white",size=1.5)

ggplot(data=TEST) +
  geom_point(aes(x=neg_rate_prop,y=price),color="lightskyblue2") + theme_dark() +
  geom_abline(aes(intercept=2.22,slope=7.56),color="white",size=1.5)
```
```{r}
MODEL1B = function(DATA,COEF){
  FIT=COEF[1]+COEF[2]*DATA$release_year
}

MSE1B=function(DATA,COEF){
  ERROR=DATA$price-MODEL1B(DATA,COEF)
  LOSS=mean(ERROR^2)
  return(LOSS)
}

MAE1B=function(DATA,COEF){
  ERROR=DATA$price-MODEL1B(DATA,COEF)
  LOSS=mean(abs(ERROR))
  return(LOSS)
}

BESTMSE1B=optim(par=c(0,0),fn=MSE1B,DATA=TRAIN)
BESTMSE1B$par
BESTMAE1B=optim(par=c(0,0),fn=MAE1B,DATA=TRAIN)
BESTMAE1B$par

#Bo and B1
  
TRAIN %>%
  filter(price<100) %>%
  ggplot()+ geom_point(aes(x=release_year,y=price),color="lightskyblue2") +
    geom_abline(aes(intercept=441,slope=-0.215),color="white",size=1.5)+theme_dark()

TEST %>%
  filter(price<100) %>%
  ggplot()+ geom_point(aes(x=release_year,y=price),color="lightskyblue2") +
    geom_abline(aes(intercept=441,slope=-0.215),color="white",size=1.5)+theme_dark()

```
```{r}

LM2=lm(price~neg_rate_prop+release_year,data=TRAIN)
summary(LM2)
summary(LM2)$sigma #The RSE value is 8.044, however a small RSE can correspond to a large error if the range of prices is quite variable.

MSE2=function(DATA,COEF){
  ERROR=DATA$price-MODEL2(DATA,COEF)
  LOSS=mean(ERROR^2)
  return(LOSS)
}
MAE2=function(DATA,COEF){
  ERROR=DATA$price-MODEL2(DATA,COEF)
  LOSS=mean(abs(ERROR))
  return(LOSS)
}


ggplot(TRAIN, aes(x = release_year, y = price)) +
  geom_point(aes(color = neg_rate_prop),size=3) +
  scale_color_gradient2(low="blue",mid="white",high="green",midpoint=0.5) +
  labs(title = "TRAIN DATA Release Year vs. Price", x = "Release Year", y = "Price", color = "Negative Rate Proportion") +
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    axis.line = element_line(size = 1),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14)
  )

residuals_df <- data.frame(Residuals = resid(LM2), Predicted = predict(LM2))

# Plot the residuals against the predicted values
ggplot(residuals_df, aes(x = Predicted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "Residual Plot", x = "Predicted Values", y = "Residuals")

#Based off the residual plot, we can tell that the model is overpredicting the price. This can be explained through the dense aggregation of data points above the y=0 residual line.


#generate predicted values on test set
test_preds <- predict(LM2, newdata = TEST)
residuals_df <- data.frame(Residuals = TEST$price - test_preds, Predicted = test_preds)

ggplot(residuals_df, aes(x = Predicted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "Residual Plot (Test Set)", x = "Predicted Values", y = "Residuals")



MODEL2 = function(DATA,COEF){
  FIT=COEF[1]+COEF[2]*DATA$neg_rate_prop+COEF[3]*DATA$release_year
}

MODEL2.GRAPH = sgf %>% 
  mutate(predict.price=MODEL2(DATA=sgf,COEF=coef(LM2))) %>%
  ggplot()+geom_point(aes(x=price,y=predict.price,
              color=factor(SPLIT,levels=c("TRAIN","TEST"))),alpha=0.2) + 
  theme_minimal() + geom_abline(intercept=0,slope=1) +
  guides(color=guide_legend(title="Dataset")) + xlab("Price") +
  ylab("Predicted Price")
  
```


```{r}
MODELS=c("MODEL 0","MODEL 1A","MODEL 1B","MODEL 2")
MSE=c(MSE0(TEST,c(4.26)),
      MSE1A(TEST,c(2.22,7.56)),
      MSE1B(TEST,c(441,-0.22)),
      MSE2(TEST,c(371.5,-2.53,-0.18)))
MAE=c(MAE0(TEST,c(4.26)),
      MAE1A(TEST,c(2.22,7.56)),
      MAE1B(TEST,c(441,-0.22)),
      MAE2(TEST,c(371.5,-2.53,-0.18)))
COMPARE=tibble(MODELS=MODELS,MSE=MSE,MAE=MAE)
print(COMPARE)
```

##RIDGE REGRESSION USING THREE-PREDICTOR VARIABLES

```{r}
library(glmnet)
response_var <- steam_games_final$price
predictor_vars <- steam_games_final[,c("neg_rate_prop","release_year","release_month")]
predictor_vars.MATRIX <- as.matrix(predictor_vars)
ridge_mod <- glmnet(x=predictor_vars.MATRIX,y=response_var,alpha=0)
plot(ridge_mod,xvar="lambda")

#As the value of log lambda increases, the penalty term in the objective function becomes more dominant, and the coefficients are shrunk more towards zero. This is why the coefficients converge towards zero as log lambda nears 6

#The fact that the three lines merge as log lambda nears 6 and the coefficients near 0 means that the three predictor variables may not be contributing much to the model's predictive power, and thus their coefficients are shrunk towards zero. This suggests that the model may be overfitting the data if all three predictors are included. To avoid overfitting, it may be necessary to remove some predictors or use a different type of regularization such as Lasso regression.

#In some cases, having a wider range of coefficients can indicate that some predictors are more influential in predicting the outcome variable than others. However, it is important to note that the magnitude of coefficients can also be influenced by the scale of the predictor variables. For example, if one predictor variable is measured in thousands and another in ones, the coefficients for the former variable will be larger, even if it is not necessarily more influential in predicting the outcome variable.

#HOW CAN WE USE THIS FOR PREDICTIONS?

#Ridge regression can be used for both prediction and variable selection. It can help prevent overfitting by shrinking the coefficients of the predictor variables towards zero, and can therefore reduce the variance of the predictions.

cv.obj <- cv.glmnet(x=predictor_vars.MATRIX,y=as.matrix(response_var), alpha = 0, nfolds = 10)
optimal_lambda <- cv.obj$lambda.min #this cross-validation was performed to isolate the optimal lambda value using three predictor variables 

ridge_coef <- coef(ridge_mod,s=0.06) #coef() function is used to obtain the coefficients for the ridge regression with lambda = 0.06.

#intercept = 400, beta1 = -2.55, beta2 = -0.19, beta3 = -0.002



train.model.func <- function(data, beta0, beta1, beta2, beta3) {
  model <- lm(price ~ neg_rate_prop + release_year + release_month, data = data, 
              lambda = 0.06, intercept = beta0, coef = c(beta1, beta2, beta3))
  return(model)
}

BEST.MOD <- steam_games_final %>% 
  mutate(best.model = predict(train.model.func(data = ., beta0 = 399.221596204, 
                                                beta1 = -2.559058954, beta2 = 
                                                 -0.194593524, 
                                                beta3 = -0.001589672)))

ggplot(filter(BEST.MOD,price<50), aes(x = price, y = best.model)) +
  geom_point() +
  xlab("Actual Price") + ylab("Fitted Price") +
  geom_abline(intercept = 0, slope = 1, color = "red")

```

